Orla Agentic AI Working Group                                  Orla Project
Request for Comments: 4                                        December 2025
Category: Standards Track


                    RFC 4: Orla Agent Mode Specification

Status of This Memo

   This document specifies the Agent Mode for the Orla project community.
   This memo provides information for the Orla community.  It does not
   specify an Internet standard of any kind.  Distribution of this memo
   is unlimited.

Abstract

   This document specifies Orla Agent Mode, a local-first terminal agent
   that accepts natural-language prompts and automatically invokes MCP
   tools to accomplish user tasks.  Agent Mode transforms Orla from a
   headless MCP server into a human-facing CLI agent that reasons over
   and executes tools via the Model Context Protocol.  The agent works
   out of the box with local LLMs (Ollama, llama.cpp) and optionally
   supports cloud-based models (OpenAI, Anthropic).  This RFC defines
   the command-line interface, agent architecture, model integration,
   tool orchestration, and user experience for Orla's agent capabilities.

   This RFC extends RFC 1 (Orla Runtime) and RFC 3 (Tool Package System),
   introducing agent-oriented commands as the primary interface for Orla.
   MCP server mode (orla serve) remains available for integration with
   external MCP clients.

Table of Contents

   1. Introduction
   2. Terminology
   3. Design Goals
   4. Command-Line Interface
   5. Agent Architecture
   6. Model Integration
   7. Tool Orchestration
   8. User Experience
   9. Configuration
   10. Security Considerations
   11. Relationship to RFC 1 and RFC 3
   12. Future Work
   13. References

1. Introduction

   Orla currently operates as an MCP server runtime (RFC 1) that
   discovers and executes tools, with a package management system (RFC
   3) for tool distribution.  While this enables integration with
   external MCP clients (Claude Desktop, Goose, etc.), it does not
   provide a direct human-facing interface for using Orla's tool
   ecosystem.

   RFC 4 introduces Agent Mode, which adds agent-oriented commands that
   allow users to interact with Orla directly via natural language.
   Users can execute one-shot prompts or engage in interactive chat
   sessions, with Orla automatically selecting and invoking appropriate
   tools to accomplish tasks.

1.1. Motivation

   Developers want AI agents that can actually do things, not just chat.
   The current ecosystem is crowded with code-only agents, cloud-first
   assistants, and tools that require manual tool schemas.  Orla's
   differentiator is auto-discovered tools via MCP, local-first operation,
   and zero-configuration simplicity.

   What's missing is a human-facing CLI agent that exposes this power
   cleanly.  Agent Mode addresses this by providing:

   - Natural-language interaction with Orla's tool ecosystem
   - Local-first operation (no API keys required by default)
   - Automatic tool discovery and invocation
   - Zero-to-magic in under 60 seconds

1.2. Terminology

   Agent Mode: The operational mode where Orla acts as a terminal agent,
   accepting user prompts and orchestrating tool execution.

   MCP Server Mode: The operational mode where Orla runs as a headless
   MCP server (RFC 1), exposing tools to external MCP clients.

   Tool: An executable discovered by Orla that can be invoked via MCP
   (as defined in RFC 1 and RFC 3).

   Model: A language model (local or remote) that processes user prompts
   and generates tool-calling decisions.

   Tool Call: An MCP tool execution request generated by the agent model
   and executed by Orla's runtime.

2. Design Goals

   Agent Mode is designed to meet the following goals:

   1. Zero-to-magic in under 60 seconds: Users should be able to install
      Orla and execute their first agent command without complex setup.

   2. Local-first by default: Work out of the box with local LLMs
      (Ollama, llama.cpp) without requiring API keys.

   3. Dead-simple tooling: Auto-discovered tools require no JSON schemas
      or boilerplate.

   4. Safe and transparent: Provide dry-run mode, confirmation for
      destructive actions, and clear display of invoked tools.

   5. Standards-based: Use MCP for tool communication, enabling tools to
      work in both agent mode and server mode.

3. Command-Line Interface

3.1. Agent Commands

   Orla MUST support the following agent-oriented commands:

      orla agent <prompt>

   Execute a one-shot agent prompt.  Orla processes the prompt, selects
   and invokes appropriate tools, and returns the result.  This command
   MUST support streaming output by default.  The prompt is provided as
   a single argument.  When the prompt contains spaces, users MUST quote
   it in the shell (e.g., `orla agent "list files"`), but from the
   program's perspective, it is received as a single string argument.

   Examples:
      orla agent "list files in the current directory"
      orla agent "generate a Dockerfile for this repo"
      orla agent --model ollama:llama3 "explain this code"

      orla chat

   Start an interactive REPL session.  Users can engage in multi-turn
   conversations with the agent, with conversation history maintained
   across turns.  The session continues until the user exits (Ctrl+D or
   /exit command).

3.2. Tool Management Commands

   Tool management commands MUST be organized under the `orla tool`
   namespace:

      orla tool list

   List all installed tools with their versions and descriptions.

      orla tool install <name>[@version]

   Install a tool from the registry (RFC 3).  Version is optional;
   defaults to "latest".

      orla tool uninstall <name>

   Remove an installed tool.

      orla tool search <query>

   Search the registry for tools matching the query.

      orla tool info <name>

   Display detailed information about a tool, including description,
   version, dependencies, and permissions.

      orla tool update <name>

   Update a tool to the latest version.

3.3. Configuration Commands

      orla config get <key>

   Display the value of a configuration key.

      orla config set <key>=<value>

   Set a configuration key to a value.

      orla config list

   Display all configuration keys and values.

3.4. Server Mode Command

      orla serve [options]

   Run Orla as an MCP server (RFC 1).  This mode exposes Orla's tools
   to external MCP clients (Claude Desktop, Goose, etc.) via HTTP or
   stdio transport.  Options include --port, --stdio, --config,
   --tools-dir, and --pretty.

3.5. Examples

      orla tool install fs
      orla agent "list files in the current directory"
      orla agent "generate a Dockerfile for this repo"
      orla agent --model ollama:llama3 "explain this code"
      orla chat

   Note: The quotes in the examples are for shell syntax (to handle spaces
   in the prompt).  The program receives the prompt as a single string
   argument regardless of whether quotes were used in the shell.
      orla config set model=ollama:llama3
      orla config set model=openai:gpt-4
      orla serve --stdio

4. Agent Architecture

4.1. High-Level Flow

   When a user executes `orla "<prompt>"` or `orla chat`:

   1. Orla starts an internal MCP client
   2. Orla connects to a local MCP server instance (orla serve running
      internally)
   3. The local server exposes all discovered tools (RFC 1, RFC 3)
   4. The agent model processes the user prompt
   5. The model generates tool-calling decisions via MCP
   6. Orla executes tool calls and returns results to the model
   7. The model synthesizes a final response
   8. Orla streams the response to the user

4.2. Internal MCP Server

   Agent Mode MUST use Orla's existing MCP server (RFC 1) internally.
   The server runs in-process and exposes all tools discovered via:

   - Directory-based discovery (RFC 1): ./tools/ directory
   - Installed tools (RFC 3): tools_dir/TOOL-NAME/VERSION/ (project-local)
                              or ~/.orla/tools/TOOL-NAME/VERSION/ (global)

   The internal server MUST use stdio transport for communication with
   the agent model.  The server MUST be started automatically when Agent
   Mode is invoked and MUST be shut down when the agent session ends.

4.3. MCP Client

   Orla MUST implement an MCP client that:

   - Connects to the internal MCP server via stdio
   - Exposes tool discovery and execution capabilities to the agent model
   - Handles tool call results and errors
   - Manages conversation context and tool call history

4.4. Agent Loop

   The agent execution loop MUST:

   1. Receive user prompt
   2. Send prompt and available tools to the model
   3. Receive model response (may include tool calls)
   4. Execute tool calls via MCP
   5. Return tool results to the model
   6. Receive final response from the model
   7. Stream response to user
   8. (For chat mode) Maintain conversation history for next turn

5. Model Integration

5.1. Local Models (Default)

   Orla MUST support local LLMs as the default model provider.  Local
   models enable:

   - Zero-configuration operation (no API keys)
   - Privacy (data never leaves the machine)
   - Offline operation
   - Cost-free usage

5.1.1. Ollama Integration

   Orla MUST support Ollama as the primary local model provider.  Ollama
   provides a simple HTTP API for local LLM inference.

   Configuration format:

      model: ollama:<model-name>

   Examples:

      model: ollama:llama3
      model: ollama:mistral
      model: ollama:codellama

   Orla MUST:

   - Auto-detect if Ollama is running (check http://localhost:11434)
   - Support streaming responses from Ollama
   - Handle tool-calling format expected by Ollama
   - Provide helpful error messages if Ollama is not running

   Orla checks if Ollama is running via HTTP health check.  If Ollama is
   not running, Orla MUST return a clear error message with instructions
   to start Ollama manually:

   - macOS: `brew services start ollama`
   - Linux: `systemctl --user start ollama` (if configured as user
     service) or `systemctl start ollama` (if configured as system
     service)
   - Or run: `ollama serve`

   Service management is handled by the installation process, not by
   Orla at runtime.  On macOS, Homebrew manages the Ollama service via
   `brew services`.  On Linux, the Ollama service should be configured
   during installation (e.g., via systemd unit file).

   Orla SHOULD:

   - Check if the requested model is available locally
   - Automatically pull the model if it is not available (with user
     confirmation for large models)
   - Provide clear status messages about Ollama's state (running or not
     running)

5.1.2. Other Local Providers

   Orla MAY support other local model providers:

   - llama.cpp (via server mode)
   - GPT4All
   - Local OpenAI-compatible servers

   These SHOULD use the same configuration pattern:

      model: <provider>:<model-name>

5.2. Cloud Models (Optional)

   Orla MAY support cloud-based model providers for users who prefer
   them or need more powerful models:

5.2.1. OpenAI

   Configuration format:

      model: openai:<model-name>

   API keys MUST be provided via environment variables (see Section 8.3):
      - OPENAI_API_KEY (standard, recommended)
      - ORLA_OPENAI_API_KEY (Orla-specific fallback)

   Examples:

      export OPENAI_API_KEY=sk-...
      orla "prompt" --model openai:gpt-4

      export OPENAI_API_KEY=sk-...
      orla "prompt" --model openai:gpt-4-turbo

5.2.2. Anthropic

   Configuration format:

      model: anthropic:<model-name>

   API keys MUST be provided via environment variables (see Section 8.3):
      - ANTHROPIC_API_KEY (standard, recommended)
      - ORLA_ANTHROPIC_API_KEY (Orla-specific fallback)

   Examples:

      export ANTHROPIC_API_KEY=sk-ant-...
      orla "prompt" --model anthropic:claude-3-opus

      export ANTHROPIC_API_KEY=sk-ant-...
      orla "prompt" --model anthropic:claude-3-sonnet

5.3. Model Selection

   Orla MUST support model selection via:

   - Configuration file: `model: ollama:llama3`
   - Command-line flag: `orla --model openai:gpt-4 "<prompt>"`
   - Environment variable: `ORLA_MODEL=ollama:llama3`

   Configuration precedence (highest to lowest):

   1. Command-line flag
   2. Environment variable
   3. Configuration file
   4. Default (ollama:llama3 or first available Ollama model)

5.4. Tool-Calling Format

   Orla MUST communicate tool schemas to the model in a format the model
   understands.  For OpenAI-compatible models, this is the standard
   function-calling format.  For other models, Orla MUST adapt the tool
   schema format as needed.

   Tool schemas MUST be derived from:

   - tool.yaml manifests (RFC 3): mcp.input_schema, mcp.output_schema
   - Executable discovery (RFC 1): Inferred from tool execution patterns

6. Tool Orchestration

6.1. Tool Discovery

   Agent Mode MUST discover tools using the same mechanisms as MCP
   Server Mode:

   - Directory-based discovery (RFC 1): Scan ./tools/ for executables
   - Installed tools (RFC 3): Scan tools_dir/TOOL-NAME/VERSION/ (project-local)
                              or ~/.orla/tools/TOOL-NAME/VERSION/ (global) for
     tool.yaml manifests

   All discovered tools MUST be exposed to the agent model via MCP.

6.2. Tool Execution

   Tool execution MUST use Orla's existing tool executor (RFC 1):

   - Simple mode: Execute on-demand per request
   - Capsule mode: Use lifecycle management (RFC 3 runtime.mode)

   Tool execution MUST respect:

   - Timeout configuration
   - Environment variables
   - Runtime arguments
   - Permission boundaries (future)

6.3. Tool Call Display

   Orla MUST display tool calls to the user in a clear, readable format:

   - Tool name and arguments
   - Execution status (running, success, error)
   - Execution duration
   - Output preview (truncated if long)

   This transparency enables users to understand what the agent is doing
   and debug issues.

6.4. Error Handling

   When a tool call fails, Orla MUST:

   1. Display the error to the user
   2. Return the error to the agent model
   3. Allow the model to retry with different parameters or select
      alternative tools
   4. Provide helpful error messages (e.g., "Tool 'fs' timed out after
      30 seconds")

7. User Experience

7.1. Streaming Responses

   Orla MUST stream agent responses to the user as they are generated.
   This provides immediate feedback and reduces perceived latency.

   Streaming MUST include:

   - Agent text responses (token-by-token)
   - Tool call announcements
   - Tool execution progress
   - Final synthesized response

7.2. Rich Terminal Output

   Orla SHOULD provide rich terminal output when appropriate, but MUST
   automatically detect terminal capabilities and fall back to plain text
   when needed.

   Orla MUST:

   - Check if stdout is a TTY (terminal) before using rich output
   - Check if the terminal supports colors/ANSI codes
   - Automatically disable rich output when:
     * stdout is not a TTY (pipes, scripts, CI/CD)
     * Terminal doesn't support colors (TERM=dumb, etc.)
     * Running in non-interactive mode
   - Provide plain text output that works in all environments

   When rich output is enabled, Orla MAY use libraries like:

   - charmbracelet/lipgloss (Go)
   - pterm (Go)

   Rich output MAY include:

   - Color-coded tool calls
   - Progress indicators
   - Section headers
   - Syntax highlighting for code outputs
   - Truncation with "show more" for long outputs

   Users MAY explicitly control output format via:

   - Command-line flag: `orla --no-color "<prompt>"` or `orla --plain "<prompt>"`
   - Configuration: `output_format: plain` or `output_format: rich`
   - Environment variable: `ORLA_NO_COLOR=1` or `NO_COLOR=1` (standard)

   Plain text output MUST be:

   - Readable in scripts and logs
   - Compatible with SSH sessions
   - Suitable for piping to files or other commands
   - Clear and informative without visual formatting

7.3. Dry-Run Mode

   Orla SHOULD support a dry-run mode that:

   - Shows what tools would be called
   - Displays tool arguments
   - Does not execute tools
   - Useful for understanding agent behavior before execution

   Dry-run mode MAY be enabled via:

      orla --dry-run "<prompt>"

7.4. Confirmation for Destructive Actions

   Orla SHOULD detect potentially destructive tool calls (e.g., file
   deletion, system modifications) and prompt for confirmation:

      Tool 'fs' will delete file: /path/to/file
      Continue? [y/N]

   This confirmation MAY be bypassed with a --yes flag.

7.5. Tool Call Logging

   Orla SHOULD log tool calls to:

   - stdout (during execution)
   - Optional log file (if configured)

   Log entries MUST include:

   - Timestamp
   - Tool name
   - Arguments
   - Execution duration
   - Success/failure status
   - Output preview

8. Configuration

8.1. Configuration File

   Orla MUST support a configuration file for Agent Mode settings.
   Configuration file location:

   - ~/.orla/config.yaml (user-specific)
   - ./orla.yaml (project-specific, overrides user config)

8.2. Configuration Options

   Agent Mode configuration options:

      # Model configuration
      model: ollama:llama3
      # Note: Ollama must be started manually before use
      # macOS: brew services start ollama
      # Linux: systemctl --user start ollama (or systemctl start ollama)
      # Note: API keys should be provided via environment variables (see 8.3)
      # Config file values are optional and discouraged for security reasons

      # Agent behavior
      timeout: 300  # Agent execution timeout (seconds)
      max_tool_calls: 10  # Maximum tool calls per prompt
      streaming: true  # Enable streaming responses
      output_format: auto  # auto, rich, or plain (auto detects TTY/colors)

      # Tool discovery (inherited from RFC 1)
      tools_dir: ./tools

      # Logging
      log_format: pretty  # json or pretty
      log_level: info
      log_file: ""  # Optional log file path

      # Safety
      confirm_destructive: true  # Prompt for destructive actions
      dry_run: false  # Default to dry-run mode

8.3. Environment Variables

   Orla MUST support configuration via environment variables.  For API
   keys, environment variables are the RECOMMENDED method and MUST be
   checked before config file values.

   Orla MUST check the following environment variables (in order):

   For OpenAI API keys:
      - OPENAI_API_KEY (standard, used by OpenAI CLI and other tools)
      - ORLA_OPENAI_API_KEY (Orla-specific)

   For Anthropic API keys:
      - ANTHROPIC_API_KEY (standard, used by Anthropic CLI and other tools)
      - ORLA_ANTHROPIC_API_KEY (Orla-specific)

   For other configuration:
      ORLA_MODEL=ollama:llama3
      ORLA_AUTO_START_OLLAMA=true
      ORLA_AUTO_CONFIGURE_OLLAMA_SERVICE=true
      ORLA_TIMEOUT=300
      ORLA_TOOLS_DIR=./tools
      ORLA_OUTPUT_FORMAT=auto  # auto, rich, or plain
      NO_COLOR=1  # Standard environment variable to disable colors
      ORLA_NO_COLOR=1  # Orla-specific color disable

   Environment variables MUST override configuration file values.  For
   API keys, Orla MUST prioritize standard environment variable names
   (OPENAI_API_KEY, ANTHROPIC_API_KEY) over Orla-specific names
   (ORLA_OPENAI_API_KEY, ORLA_ANTHROPIC_API_KEY) to maintain
   compatibility with existing tooling and workflows.

9. Security Considerations

9.1. Tool Execution Security

   Agent Mode inherits all security considerations from RFC 1 (tool
   execution) and RFC 3 (tool installation).  Additional considerations:

   - Agent models may generate unexpected tool calls
   - Users should review tool calls before confirmation
   - Dry-run mode helps users understand agent behavior

9.2. Model API Keys

   When using cloud models, API keys MUST be stored securely:

   - API keys MUST be read from environment variables by default
   - Orla MUST check standard environment variable names first
     (OPENAI_API_KEY, ANTHROPIC_API_KEY) for compatibility
   - Config file values for API keys are optional and SHOULD be
     discouraged in documentation
   - Never log API keys in any output (stdout, stderr, log files)
   - Support keychain/credential storage (OS-specific) for future
     implementations
   - Allow key rotation without reconfiguration (via environment
     variables)
   - Provide clear error messages if API keys are missing, directing
     users to set environment variables

9.3. Local Model Security

   Local models execute on the user's machine.  Users MUST:

   - Trust the model provider (Ollama, etc.)
   - Understand that model outputs are not sandboxed
   - Review tool calls before execution

9.4. Permission Model

   Tools MAY declare required permissions in their manifests (RFC 3).
   Orla SHOULD:

   - Display permissions when installing tools
   - Warn users before executing tools with elevated permissions
   - Log permission usage for audit

   Future versions MAY implement permission enforcement and sandboxing.

10. Relationship to RFC 1 and RFC 3

10.1. RFC 1 (Orla Runtime)

   Agent Mode uses RFC 1's MCP server internally.  All tool discovery and
   execution mechanisms from RFC 1 are reused:

   - Directory-based tool discovery
   - Tool execution with timeout
   - Hot reloading (SIGHUP)
   - Configuration file format

   Agent Mode extends RFC 1 by adding a human-facing interface and model
   integration layer.

10.2. RFC 3 (Tool Package System)

   Agent Mode uses RFC 3's tool installation and registry system:

   - orla tool install/list/search commands
   - Tool package format (tool.yaml)
   - Registry architecture
   - Version management

   Agent Mode makes installed tools immediately available for agent
   execution without requiring external MCP clients.

10.3. MCP Server Mode Compatibility

   Agent Mode and MCP Server Mode share the same underlying
   infrastructure:

   - Both modes use the same tool discovery mechanisms (RFC 1, RFC 3)
   - Both modes use the same tool execution engine
   - Configuration file format is shared between modes
   - Tool packages work in both agent mode and server mode
   - Users can run `orla serve` to expose tools to external MCP clients

11. Future Work

   The following features are explicitly out of scope for v0 but MAY be
   added in future revisions:

   1. Multi-agent coordination
   2. Long-running agent tasks with persistence
   3. Agent memory and learning from past interactions
   4. Custom agent personalities/prompts
   5. Agent-to-agent communication
   6. Distributed agent execution
   7. Advanced planning and goal decomposition
   8. Tool call optimization and batching
   9. Agent performance metrics and analytics
   10. Integration with external agent frameworks

12. References

12.1. Normative References

   [RFC1]  Orla Project, "RFC 1: Orla Runtime Specification",
          December 2025.

   [RFC3]  Orla Project, "RFC 3: Tool Package and Install System",
          December 2025.

   [MCP]  Model Context Protocol Specification,
          https://modelcontextprotocol.io/specification

   [RFC2119]
          Bradner, S., "Key words for use in RFCs to Indicate
          Requirement Levels", BCP 14, RFC 2119,
          DOI 10.17487/RFC2119, March 1997,
          <https://www.rfc-editor.org/info/rfc2119>.

12.2. Informative References

   [Ollama]
          Ollama Project, "Run large language models locally",
          <https://ollama.ai>.

   [OpenAI]
          OpenAI, "OpenAI API Documentation",
          <https://platform.openai.com/docs>.

   [Anthropic]
          Anthropic, "Anthropic API Documentation",
          <https://docs.anthropic.com>.

Author's Address

   Orla Project
   https://github.com/dorcha-inc/orla

